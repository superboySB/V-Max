# Copyright 2025 Valeo.

"""Trainer for Behavioral Cloning (BC) algorithm."""

from __future__ import annotations

import typing
from collections.abc import Callable
from functools import partial
from time import perf_counter

import jax
import jax.numpy as jnp
from tqdm import tqdm

from vmax.learning import datatypes, pipeline
from vmax.learning.algorithms.imitation import bc
from vmax.learning.algorithms.replay_buffer import ReplayBuffer
from vmax.learning.pipeline import pmap
from vmax.scripts.training import train_utils
from vmax.simulator import metrics as _metrics
from vmax.simulator.wrappers import action


if typing.TYPE_CHECKING:
    from waymax import datatypes as waymax_datatypes
    from waymax import env as waymax_env


def train(
    env: waymax_env.PlanningAgentEnvironment,
    data_generator: typing.Iterator[waymax_datatypes.SimulatorState],
    eval_scenario: waymax_datatypes.SimulatorState,
    num_scenario_per_eval: int,
    total_timesteps: int,
    num_envs: int,
    num_episode_per_epoch: int,
    scenario_length: int,
    log_freq: int,
    seed: int,
    save_freq: int,
    eval_freq: int,
    buffer_size: int,
    batch_size: int,
    learning_rate: float,
    loss_type: str,
    grad_updates_per_step: int,
    unroll_length: int,
    network_config: dict,
    progress_fn: Callable[[int, datatypes.Metrics], None] = lambda *args: None,
    checkpoint_logdir: str = "",
) -> None:
    """Train a Behavioral Cloning (BC) model.

    Args:
        env: Environment object.
        data_generator: Iterator that yields simulator states.
        eval_scenario: Scenario used for evaluation.
        num_scenario_per_eval: Number of evaluation scenarios.
        total_timesteps: Total number of timesteps for training.
        num_envs: Number of parallel environments.
        num_episode_per_epoch: Number of episodes per epoch.
        scenario_length: Length of each scenario.
        log_freq: Frequency for logging metrics.
        seed: Random seed for reproducibility.
        learning_start: Timestep to begin learning.
        save_freq: Frequency of saving model checkpoints.
        eval_freq: Frequency of evaluation.
        buffer_size: Capacity of the replay buffer.
        batch_size: Batch size for training.
        learning_rate: Learning rate for updating the model.
        loss_type: Identifier for the loss function.
        grad_updates_per_step: Number of gradient updates per step.
        unroll_length: Unroll length for each training iteration.
        network_config: Configuration dictionary for the model network.
        progress_fn: Callback function for reporting progress.
        checkpoint_logdir: Directory path for storing checkpoints.

    """
    rng = jax.random.PRNGKey(seed)
    num_devices = jax.local_device_count()

    do_save = save_freq > 1 and checkpoint_logdir is not None
    do_evaluation = eval_freq >= 1

    start_memory = train_utils.get_memory_usage()

    num_steps = num_episode_per_epoch * scenario_length
    env_steps_per_iter = num_steps * num_envs
    total_iters = (total_timesteps // env_steps_per_iter) + 1

    dummy_scenario = next(data_generator)
    dummy_scenario = jax.tree.map(lambda x: x[dummy_scenario.shape[:-1]], dummy_scenario)

    rng, subkey = jax.random.split(rng, 2)
    scenario_key = jax.random.split(subkey, dummy_scenario.shape)
    dummy_scenario = env.reset(dummy_scenario, scenario_key)

    observation_size = env.observation_spec(dummy_scenario.state)
    action_size = env.action_spec().data.shape[0]

    print("shape check".center(50, "="))
    print(f"observation size: {observation_size}")
    print(f"action size: {action_size}")

    rng, network_key = jax.random.split(rng)

    network, training_state, policy_fn = bc.initialize(
        action_size,
        observation_size,
        env,
        learning_rate,
        network_config,
        num_devices,
        network_key,
    )
    learning_fn = bc.make_sgd_step(network, loss_type)
    step_fn = partial(action.expert_step, use_partial_transition=True)

    replay_buffer = ReplayBuffer(
        buffer_size=buffer_size // num_devices,
        batch_size=batch_size * grad_updates_per_step // num_devices,
        samples_size=num_envs // num_devices,
        dummy_data_sample=datatypes.RLPartialTransition(
            observation=jnp.zeros((observation_size,)),
            action=jnp.zeros((action_size,)),
            reward=0.0,
            flag=0,
            done=0,
        ),
    )

    unroll_fn = partial(
        action.generate_unroll,
        unroll_length=unroll_length,
        env=env,
        step_fn=step_fn,
    )

    run_training = partial(
        pipeline.run_training_off_policy,
        replay_buffer=replay_buffer,
        env=env,
        learning_fn=learning_fn,
        policy_fn=policy_fn,
        unroll_fn=unroll_fn,
        grad_updates_per_step=grad_updates_per_step,
        scan_length=num_steps // unroll_length,
    )
    run_evaluation = partial(
        pipeline.run_evaluation,
        env=env,
        policy_fn=policy_fn,
        step_fn=action.expert_step,
        scan_length=scenario_length * num_scenario_per_eval,
    )

    run_training = jax.pmap(run_training, axis_name="batch")
    run_evaluation = jax.pmap(run_evaluation, axis_name="batch")

    rng, rb_key = jax.random.split(rng)
    buffer_state = jax.pmap(replay_buffer.init)(jax.random.split(rb_key, num_devices))

    current_memory = train_utils.get_memory_usage()
    print(f"Memory usage: {current_memory:.2f} MB")
    print(f"Memory usage increase: {(current_memory - start_memory) / 1024**3:.2f} GB")
    start_memory = current_memory

    time_training = perf_counter()

    current_step = 0

    print("training".center(50, "="))
    for iter in tqdm(range(total_iters), desc="Training", total=total_iters, dynamic_ncols=True):
        rng, iter_key = jax.random.split(rng)
        iter_keys = jax.random.split(iter_key, num_devices)

        # Batch data generation
        t = perf_counter()
        batch_scenarios = next(data_generator)
        epoch_data_time = perf_counter() - t

        # Training step
        t = perf_counter()
        training_state, buffer_state, training_metrics = run_training(
            batch_scenarios,
            training_state,
            buffer_state,
            iter_keys,
        )
        jax.tree_util.tree_map(lambda x: x.block_until_ready(), training_metrics)

        epoch_training_time = perf_counter() - t

        #  Log training metrics
        t = perf_counter()
        training_metrics = pmap.flatten_tree(training_metrics)
        training_metrics = jax.device_get(training_metrics)
        training_metrics = _metrics.collect(training_metrics, "steps")

        current_step = env_steps_per_iter * (iter + 1)

        metrics = {
            "runtime/sps": int(env_steps_per_iter / epoch_training_time),
            **{f"{name}": value for name, value in training_metrics.items() if "learning" in name},
        }

        if do_save and not iter % save_freq:
            path = f"{checkpoint_logdir}/model_{current_step}.pkl"
            train_utils.save_params(path, pmap.unpmap(training_state.params))

        epoch_log_time = perf_counter() - t

        # Evaluation
        t = perf_counter()
        if do_evaluation and not iter % eval_freq:
            eval_metrics = run_evaluation(eval_scenario, training_state)
            jax.tree_util.tree_map(lambda x: x.block_until_ready(), eval_metrics)
            eval_metrics = pmap.flatten_tree(eval_metrics)
            eval_metrics = _metrics.collect(eval_metrics, "eval/steps")
            progress_fn(current_step, eval_metrics)

        epoch_eval_time = perf_counter() - t

        if not iter % log_freq:
            metrics["runtime/memory_usage"] = train_utils.get_memory_usage()
            metrics["runtime/data_time"] = epoch_data_time
            metrics["runtime/training_time"] = epoch_training_time
            metrics["runtime/log_time"] = epoch_log_time
            metrics["runtime/eval_time"] = epoch_eval_time
            metrics["runtime/iter_time"] = epoch_data_time + epoch_training_time + epoch_log_time + epoch_eval_time
            metrics["runtime/wall_time"] = perf_counter() - time_training

            progress_fn(current_step, metrics, current_step, total_timesteps)

    print(f"-> Training took {perf_counter() - time_training:.2f}s")
    assert current_step >= total_timesteps

    if checkpoint_logdir:
        path = f"{checkpoint_logdir}/model_final.pkl"
        train_utils.save_params(path, pmap.unpmap(training_state.params))

    pmap.assert_is_replicated(training_state)
    pmap.synchronize_hosts()
